#!/usr/bin/env python3
"""
ìˆ˜í•™ê³¼ êµìœ¡ê³¼ì • PDF ì§ì ‘ ì¸ë±ì‹± ìŠ¤í¬ë¦½íŠ¸

ì‹¤í–‰ ë°©ë²•:
    python scripts/index_math_direct.py
"""

import asyncio
import sys
from pathlib import Path
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from backend.app.services.rag.parser_service import ParserService
from backend.app.services.rag.embedding_service import EmbeddingService
from backend.app.services.rag.vector_store import VectorStore


# í•™ë…„ ë§¤í•‘
GRADE_MAPPING = {
    "2": "ì´ˆ1~2",
    "4": "ì´ˆ3~4",
    "6": "ì´ˆ5~6",
    "9": "ì¤‘1~3",
    "10": "ê³ ë“±í•™êµ",
    "12": "ê³ ë“±í•™êµ"
}

# ì˜ì—­ í‚¤ì›Œë“œ
DOMAIN_KEYWORDS = {
    "ìˆ˜ì™€ ì—°ì‚°": ["ìˆ˜", "ì—°ì‚°", "ë§ì…ˆ", "ëº„ì…ˆ", "ê³±ì…ˆ", "ë‚˜ëˆ—ì…ˆ", "ë¶„ìˆ˜", "ì†Œìˆ˜", "ìì—°ìˆ˜"],
    "ë„í˜•": ["ë„í˜•", "ì‚¼ê°í˜•", "ì‚¬ê°í˜•", "ì›", "ê°", "ì„ ë¶„", "í‰ë©´ë„í˜•", "ì…ì²´ë„í˜•"],
    "ì¸¡ì •": ["ì¸¡ì •", "ê¸¸ì´", "ë“¤ì´", "ë¬´ê²Œ", "ì‹œê°„", "ë„“ì´", "ë¶€í”¼"],
    "ê·œì¹™ì„±": ["ê·œì¹™", "íŒ¨í„´", "í•¨ìˆ˜", "ë°©ì •ì‹", "ì‹", "ëŒ€ìˆ˜"],
    "ìë£Œì™€ ê°€ëŠ¥ì„±": ["ìë£Œ", "ê·¸ë˜í”„", "í‘œ", "í™•ë¥ ", "í†µê³„", "ê²½ìš°ì˜ ìˆ˜"]
}


def extract_grade_from_code(code: str) -> str:
    """ì„±ì·¨ê¸°ì¤€ ì½”ë“œì—ì„œ í•™ë…„ ì¶”ì¶œ"""
    if not code:
        return "ê¸°íƒ€"
    
    # [2ìˆ˜01-01] â†’ "2"
    grade_num = code[1] if len(code) > 1 else ""
    return GRADE_MAPPING.get(grade_num, "ê¸°íƒ€")


def extract_domain_from_content(content: str) -> str:
    """ë‚´ìš©ì—ì„œ ì˜ì—­ ì¶”ì¶œ"""
    for domain, keywords in DOMAIN_KEYWORDS.items():
        if any(kw in content for kw in keywords):
            return domain
    return "ì¼ë°˜"


async def index_math_curriculum():
    """ìˆ˜í•™ê³¼ êµìœ¡ê³¼ì • ì§ì ‘ ì¸ë±ì‹±"""
    
    print("=" * 70)
    print("ìˆ˜í•™ê³¼ êµìœ¡ê³¼ì • PDF â†’ RAG ì¸ë±ì‹±")
    print("=" * 70)
    print()
    
    # 1. ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
    print("1ï¸âƒ£  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
    parser = ParserService()
    embedding_service = EmbeddingService(use_ollama=True)
    vector_store = VectorStore()
    
    await vector_store.initialize_collection()
    print("   âœ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
    print()
    
    # 2. íŒŒì¼ íŒŒì‹±
    pdf_path = Path("asset/[ë³„ì±…8]+ìˆ˜í•™ê³¼+êµìœ¡ê³¼ì •.pdf")
    
    if not pdf_path.exists():
        print(f"   âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        return
    
    metadata = {
        "policy_version": "2022ê°œì •",
        "scope_type": "NATIONAL",
        "subject": "ìˆ˜í•™",
        "document_type": "ì„±ì·¨ê¸°ì¤€"
    }
    
    print(f"2ï¸âƒ£  PDF íŒŒì‹± ì¤‘... ({pdf_path.name})")
    chunks = await parser.parse_document(pdf_path, "curriculum", metadata)
    print(f"   âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
    print()
    
    # 3. ë©”íƒ€ë°ì´í„° ë³´ê°•
    print("3ï¸âƒ£  ë©”íƒ€ë°ì´í„° ë³´ê°• ì¤‘...")
    for chunk in chunks:
        # ì„±ì·¨ê¸°ì¤€ ì½”ë“œì—ì„œ í•™ë…„ ì¶”ì¶œ
        curriculum_code = chunk.metadata.get("curriculum_code", "")
        if curriculum_code:
            grade = extract_grade_from_code(curriculum_code)
            chunk.metadata["grade_level"] = grade
        
        # ë‚´ìš©ì—ì„œ ì˜ì—­ ì¶”ì¶œ
        domain = extract_domain_from_content(chunk.content)
        chunk.metadata["domain"] = domain
    
    print(f"   âœ… ë©”íƒ€ë°ì´í„° ë³´ê°• ì™„ë£Œ")
    print()
    
    # 4. ìƒ˜í”Œ ì²­í¬ ì¶œë ¥
    print("4ï¸âƒ£  ìƒ˜í”Œ ì²­í¬ (ì²˜ìŒ 3ê°œ):")
    print("-" * 70)
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n[ì²­í¬ {i}]")
        print(f"  ë‚´ìš©: {chunk.content[:100]}...")
        print(f"  ë©”íƒ€ë°ì´í„°: {chunk.metadata}")
    print("-" * 70)
    print()
    
    # 5. ì„ë² ë”© ìƒì„±
    print("5ï¸âƒ£  ì„ë² ë”© ìƒì„± ì¤‘... (Ollama ì‚¬ìš©)")
    print(f"   ë°°ì¹˜ í¬ê¸°: 10ê°œì”©")
    
    embeddings = await embedding_service.embed_batch(
        [c.content for c in chunks],
        batch_size=10
    )
    print(f"   âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    print(f"   ë²¡í„° ì°¨ì›: {len(embeddings[0])}")
    print()
    
    # 6. ë²¡í„° DB ì €ì¥
    print("6ï¸âƒ£  ë²¡í„° DB ì €ì¥ ì¤‘...")
    batch_data = []
    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
        chunk_id = f"math_2022_{i:04d}"
        batch_data.append((
            chunk_id,
            embedding,
            chunk.metadata,
            chunk.content
        ))
    
    await vector_store.upsert_batch(batch_data)
    print(f"   âœ… {len(batch_data)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
    print()
    
    # 7. í†µê³„ ì¶œë ¥
    print("=" * 70)
    print("ğŸ“Š ì¸ë±ì‹± í†µê³„")
    print("=" * 70)
    print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
    print(f"ë²¡í„° ì°¨ì›: {len(embeddings[0])}")
    print(f"í‰ê·  ì²­í¬ í¬ê¸°: {sum(len(c.content) for c in chunks) // len(chunks)} ë¬¸ì")
    
    # í•™ë…„ë³„ í†µê³„
    grade_stats = {}
    for chunk in chunks:
        grade = chunk.metadata.get("grade_level", "ê¸°íƒ€")
        grade_stats[grade] = grade_stats.get(grade, 0) + 1
    
    print(f"\ní•™ë…„ë³„ ì²­í¬ ìˆ˜:")
    for grade, count in sorted(grade_stats.items()):
        print(f"  {grade}: {count}ê°œ")
    
    # ì˜ì—­ë³„ í†µê³„
    domain_stats = {}
    for chunk in chunks:
        domain = chunk.metadata.get("domain", "ì¼ë°˜")
        domain_stats[domain] = domain_stats.get(domain, 0) + 1
    
    print(f"\nì˜ì—­ë³„ ì²­í¬ ìˆ˜:")
    for domain, count in sorted(domain_stats.items()):
        print(f"  {domain}: {count}ê°œ")
    
    print()
    print("=" * 70)
    print("ğŸ‰ ì¸ë±ì‹± ì™„ë£Œ!")
    print("=" * 70)
    print()
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. ì„œë²„ ì‹¤í–‰: uvicorn backend.app.main:app --reload")
    print("  2. Swagger UI: http://localhost:8000/docs")
    print("  3. í…ŒìŠ¤íŠ¸ ì§ˆì˜: POST /api/v1/rag/query")
    print()


if __name__ == "__main__":
    try:
        asyncio.run(index_math_curriculum())
    except KeyboardInterrupt:
        print("\n\nì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\n\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
