# ğŸ“Š Test Report Generation Pipeline Guide

## Overview

This document describes the complete automated test report generation pipeline used in MATHESIS LAB. The pipeline can generate comprehensive test reports with metadata automatically using LLM models.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Test Execution Layer                          â”‚
â”‚  Backend (pytest) â”‚ Frontend (vitest) â”‚ E2E (playwright)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚                                                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Aggregate Test Results (JSON)      â”‚
         â”‚  - Backend: test-results.xml        â”‚
         â”‚  - Frontend: coverage reports       â”‚
         â”‚  - E2E: playwright-report, screenshots
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  (NEW) Metadata Generation (LLM-based)             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ Input: Test results JSON + Report title       â”‚ â”‚
    â”‚  â”‚ LLM: Claude/GPT-4/Gemini analyzes results     â”‚ â”‚
    â”‚  â”‚ Output: report_metadata.json                  â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚  Metadata sections generated:                        â”‚
    â”‚  1. âš ï¸  Risk Assessment & Untested Areas            â”‚
    â”‚  2. ğŸ“ˆ Performance Benchmarking                      â”‚
    â”‚  3. ğŸ“¦ Deployment Notes & Dependencies              â”‚
    â”‚  4. ğŸ› ï¸  Technical Debt & Follow-ups                â”‚
    â”‚  5. âœ… Pre-Deployment Validation Checklist         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Report Generation (test_report_generator.py)        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚ 1. Load test results                          â”‚ â”‚
    â”‚  â”‚ 2. Load metadata from JSON                    â”‚ â”‚
    â”‚  â”‚ 3. Generate markdown with metadata sections   â”‚ â”‚
    â”‚  â”‚ 4. Validate images (PIL)                      â”‚ â”‚
    â”‚  â”‚ 5. Convert to PDF with embedded images        â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Output: Test Report                                â”‚
    â”‚  test_reports/Report_Title__TIMESTAMP/              â”‚
    â”‚  â”œâ”€â”€ README.md (24KB with metadata)                â”‚
    â”‚  â”œâ”€â”€ README.pdf (1.2MB with images)                â”‚
    â”‚  â””â”€â”€ screenshots/ (25+ E2E test screenshots)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Current Status (Manual Metadata)

Currently, `tools/report_metadata.json` is created manually with 4 core non-code elements:

```json
{
  "risks_and_untested_areas": { ... },
  "performance_benchmarking": { ... },
  "dependencies_and_deployment_notes": { ... },
  "technical_debt_and_followups": { ... },
  "validation_checklist": { ... }
}
```

## Step 1: Run Tests

### Backend Tests
```bash
cd /mnt/d/progress/MATHESIS\ LAB
source .venv/bin/activate
PYTHONPATH=/mnt/d/progress/MATHESIS\ LAB pytest backend/tests/ -v --junit-xml=test-results.xml
```

### Frontend Tests
```bash
cd MATHESIS-LAB_FRONT
npm test -- --run --coverage
```

### E2E Tests
```bash
cd MATHESIS-LAB_FRONT
npx playwright test e2e/ --reporter=html
```

## Step 2: Generate Metadata (Current: Manual)

Edit `tools/report_metadata.json` with:
- Risk assessment from test failures
- Performance metrics from test execution
- Deployment checklist based on changes
- Technical debt items discovered
- Validation checklist for deployment

**JSON Schema:** See `/tools/report_metadata.json` for complete structure

## Step 3: Generate Report

```bash
cd /mnt/d/progress/MATHESIS\ LAB
source .venv/bin/activate
python tools/test_report_generator.py --title "Your Report Title"
```

**Output:**
- `test_reports/Your_Report_Title__TIMESTAMP/README.md`
- `test_reports/Your_Report_Title__TIMESTAMP/README.pdf`
- `test_reports/Your_Report_Title__TIMESTAMP/screenshots/`

## Future: LLM-Based Metadata Generation

### Overview
Instead of manually editing `report_metadata.json`, use an LLM to automatically analyze test results and generate metadata.

### How It Works
1. Parse test results JSON
2. Send to LLM with detailed prompt
3. LLM analyzes and generates structured JSON
4. Validate JSON schema
5. Use for report generation (same as Step 3)

### Benefits
âœ… **Consistency:** Same structure for all reports
âœ… **Speed:** Automatic metadata generation
âœ… **Flexibility:** Works with any LLM (Claude, GPT-4, Gemini)
âœ… **Extensibility:** Easy to add new metadata sections

### Implementation Example

**File: `tools/generate_metadata.py`** (to be created)

```python
import json
import anthropic

def generate_metadata_with_llm(test_results_path, report_title):
    """
    Generate report_metadata.json using Claude API

    Args:
        test_results_path: Path to aggregated test results JSON
        report_title: Title of the test report

    Returns:
        dict: Generated metadata matching report_metadata.json schema
    """

    # Read test results
    with open(test_results_path) as f:
        test_results = json.load(f)

    # Initialize Anthropic client
    client = anthropic.Anthropic(api_key="your-api-key")

    # Create prompt
    prompt = f"""
    Analyze the following test results and generate comprehensive metadata
    for a test report. Return ONLY valid JSON matching the schema below.

    Report Title: {report_title}

    Test Results:
    {json.dumps(test_results, indent=2)}

    Generate JSON with these 5 sections:

    1. risks_and_untested_areas (5 items with risk_level: high/medium/low)
    2. performance_benchmarking (6 metrics with before/after values)
    3. dependencies_and_deployment_notes (deployment checklist with steps)
    4. technical_debt_and_followups (8 items with priority and effort)
    5. validation_checklist (7 verification items with PASS/FAIL status)

    Return ONLY the JSON, no markdown or explanations.
    """

    # Call Claude API
    response = client.messages.create(
        model="claude-opus-4-1",
        max_tokens=4096,
        messages=[{"role": "user", "content": prompt}]
    )

    # Parse and validate response
    metadata_json = json.loads(response.content[0].text)
    return metadata_json
```

**Usage:**
```bash
python tools/generate_metadata.py \
  --test-results aggregated-test-results.json \
  --title "My Test Report" \
  --api-key "sk-ant-..." \
  --model "claude-opus-4-1"
```

### Prompt Engineering Tips

For best results, the LLM prompt should:

1. **Specify output format clearly**
   ```
   Return ONLY valid JSON matching this schema: {...}
   Do not include markdown, explanations, or code blocks.
   ```

2. **Provide context about your project**
   ```
   This is a test report for an educational platform (MATHESIS LAB)
   with React frontend, FastAPI backend, and Playwright E2E tests.
   ```

3. **Include example metadata**
   ```
   Example risk item format:
   {
     "area": "Backend Mock Tests",
     "risk_level": "medium",
     "description": "...",
     "mitigation": "..."
   }
   ```

4. **Specify Korean text requirements** (if needed)
   ```
   Use Korean text for descriptions and titles.
   Preserve Korean text exactly in JSON output.
   ```

## Pipeline Automation (Optional)

Create `tools/auto_generate_report.sh`:

```bash
#!/bin/bash
set -e

REPORT_TITLE="${1:-Test Report $(date +%Y-%m-%d)}"
API_KEY="${ANTHROPIC_API_KEY}"

echo "ğŸ“Š Starting automated test report generation..."

# Step 1: Run all tests
echo "1ï¸âƒ£  Running backend tests..."
PYTHONPATH=/mnt/d/progress/MATHESIS\ LAB pytest backend/tests/ -v --junit-xml=test-results.xml

echo "2ï¸âƒ£  Running frontend tests..."
cd MATHESIS-LAB_FRONT
npm test -- --run --coverage
cd ..

echo "3ï¸âƒ£  Running E2E tests..."
cd MATHESIS-LAB_FRONT
npx playwright test e2e/ --reporter=html
cd ..

# Step 2: Generate metadata (LLM-based, when implemented)
echo "4ï¸âƒ£  Generating metadata with LLM..."
python tools/generate_metadata.py \
  --test-results aggregated-test-results.json \
  --title "$REPORT_TITLE" \
  --api-key "$API_KEY" \
  --model "claude-opus-4-1"

# Step 3: Generate report
echo "5ï¸âƒ£  Generating test report..."
source .venv/bin/activate
python tools/test_report_generator.py --title "$REPORT_TITLE"

echo "âœ… Report generation complete!"
echo "ğŸ“ Output: test_reports/"
```

**Usage:**
```bash
chmod +x tools/auto_generate_report.sh
ANTHROPIC_API_KEY="sk-ant-..." ./tools/auto_generate_report.sh "My Report Title"
```

## Files Reference

| File | Purpose | Format | Status |
|------|---------|--------|--------|
| `tools/test_report_generator.py` | Main report generation logic | Python | âœ… Ready |
| `tools/report_metadata.json` | Metadata for report sections | JSON | âœ… Ready |
| `tools/generate_metadata.py` | LLM-based metadata generator | Python | ğŸ“… To be created |
| `tools/auto_generate_report.sh` | Complete pipeline automation | Bash | ğŸ“… To be created |

## Key Decisions

### Why Separate Metadata File?

âœ… **Decoupling:** Test generation independent from metadata
âœ… **Flexibility:** Easy to use different LLMs
âœ… **Transparency:** Metadata visible and editable
âœ… **Version Control:** Track metadata changes in git

### Why JSON Format?

âœ… **Structured:** Consistent schema across reports
âœ… **Parseable:** Easy to read/write programmatically
âœ… **Extensible:** Easy to add new fields
âœ… **LLM-friendly:** LLMs can generate valid JSON

### Why 5 Metadata Sections?

1. **âš ï¸ Risk Assessment** â†’ Stakeholder transparency
2. **ğŸ“ˆ Performance** â†’ Technical credibility
3. **ğŸ“¦ Deployment** â†’ Operational readiness
4. **ğŸ› ï¸ Tech Debt** â†’ Future planning
5. **âœ… Validation** â†’ Quality assurance

These 5 sections provide a complete picture for decision-making.

## Troubleshooting

### JSON Schema Errors
```bash
# Validate metadata JSON
python -m json.tool tools/report_metadata.json

# Check schema against test_report_generator.py
grep -A 20 "_generate_metadata_sections" tools/test_report_generator.py
```

### LLM API Errors (when implemented)
- Check API key is valid
- Verify rate limits not exceeded
- Check network connectivity
- Review LLM error message

### Report Generation Failures
```bash
# Enable verbose output
python tools/test_report_generator.py --title "Test" --verbose

# Check file permissions
ls -la test_reports/
chmod 755 test_reports/

# Verify image files
file test_reports/*/screenshots/*.png
```

## Next Steps

1. âœ… Current: Manual metadata creation
2. ğŸ“… Phase 1: Create `generate_metadata.py` with Claude API
3. ğŸ“… Phase 2: Add support for GPT-4 and Gemini
4. ğŸ“… Phase 3: Full pipeline automation script
5. ğŸ“… Phase 4: GitHub Actions integration

---

**Last Updated:** 2025-11-16
**Maintained By:** Development Team
**Version:** 1.0
