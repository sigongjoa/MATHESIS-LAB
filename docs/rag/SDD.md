# RAG ì‹œìŠ¤í…œ ìƒì„¸ ì„¤ê³„ì„œ (SDD)

## ğŸ“‹ ëª©ì°¨
1. [ì‹œìŠ¤í…œ ê°œìš”](#1-ì‹œìŠ¤í…œ-ê°œìš”)
2. [ì•„í‚¤í…ì²˜ ìƒì„¸](#2-ì•„í‚¤í…ì²˜-ìƒì„¸)
3. [ì»´í¬ë„ŒíŠ¸ ì„¤ê³„](#3-ì»´í¬ë„ŒíŠ¸-ì„¤ê³„)
4. [ë°ì´í„° íë¦„](#4-ë°ì´í„°-íë¦„)
5. [ì˜ˆì™¸ ì²˜ë¦¬ ì „ëµ](#5-ì˜ˆì™¸-ì²˜ë¦¬-ì „ëµ)
6. [ë ˆê±°ì‹œ í†µí•©](#6-ë ˆê±°ì‹œ-í†µí•©)
7. [ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­](#7-ì„±ëŠ¥-ìš”êµ¬ì‚¬í•­)

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### 1.1 ëª©ì 
ê¸°ì¡´ MATHESIS LAB ì‹œìŠ¤í…œì— RAG(Retrieval-Augmented Generation) ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬, êµìœ¡ê³¼ì • ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ë° ë¶„ì„ ê¸°ëŠ¥ ì œê³µ

### 1.2 ë²”ìœ„
- **In-Scope**: ë¬¸ì„œ íŒŒì‹±, ë²¡í„° ê²€ìƒ‰, LLM ê¸°ë°˜ ë‹µë³€ ìƒì„±, ê·¼ê±° ì¸ìš©
- **Out-of-Scope**: ì‹¤ì‹œê°„ í˜‘ì—…, ìŒì„± ì¸í„°í˜ì´ìŠ¤, ìë™ ë²ˆì—­

### 1.3 ê¸°ìˆ  ìŠ¤íƒ

| ê³„ì¸µ | ê¸°ìˆ  | ë²„ì „ | ì´ìœ  |
|------|------|------|------|
| **Backend** | FastAPI | 0.104+ | ë¹„ë™ê¸° ì§€ì›, Pydantic í†µí•© |
| **ORM** | SQLAlchemy | 2.0+ | ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì¼ê´€ì„± |
| **Vector DB** | Qdrant | 1.7+ | ë©”íƒ€ë°ì´í„° í•„í„°ë§, ì˜¤í”ˆì†ŒìŠ¤ |
| **Embedding** | OpenAI text-embedding-3-large | - | í•œêµ­ì–´ ì„±ëŠ¥ ìš°ìˆ˜ |
| **LLM** | OpenAI GPT-4 Turbo | - | ì¶”ë¡  ëŠ¥ë ¥, í•œêµ­ì–´ ì§€ì› |
| **Parser** | PyMuPDF, python-hwp | - | PDF/HWP ì§€ì› |
| **Testing** | pytest, pytest-asyncio | - | ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ ì§€ì› |

---

## 2. ì•„í‚¤í…ì²˜ ìƒì„¸

### 2.1 ì „ì²´ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (React + TypeScript)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ NodeGraph    â”‚  â”‚ AIAssistant  â”‚  â”‚ RAGChat      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚ REST API         â”‚ REST API         â”‚ WebSocket (ì„ íƒ)
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (FastAPI)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              API Layer (v1/endpoints/)               â”‚   â”‚
â”‚  â”‚  /rag/query  /rag/index  /rag/status  /rag/feedback â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                          â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  RAG Service      â”‚      â”‚  Parser Service   â”‚          â”‚
â”‚  â”‚  - Query          â”‚      â”‚  - PDF Parser     â”‚          â”‚
â”‚  â”‚  - Retrieval      â”‚      â”‚  - HWP Parser     â”‚          â”‚
â”‚  â”‚  - Generation     â”‚      â”‚  - Metadata Ext.  â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                          â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚           Data Access Layer                  â”‚          â”‚
â”‚  â”‚  - PostgreSQL (Metadata, Logs)               â”‚          â”‚
â”‚  â”‚  - Qdrant (Vector Embeddings)                â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                          â”‚
          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚        â”‚     Qdrant      â”‚
â”‚   (ê¸°ì¡´ DB)     â”‚        â”‚  (Vector Store) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ë ˆì´ì–´ë³„ ì±…ì„

| ë ˆì´ì–´ | ì±…ì„ | ì£¼ìš” í´ë˜ìŠ¤/íŒŒì¼ |
|--------|------|-----------------|
| **API Layer** | HTTP ìš”ì²­ ì²˜ë¦¬, ì¸ì¦, ì…ë ¥ ê²€ì¦ | `backend/app/api/v1/endpoints/rag.py` |
| **Service Layer** | ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, íŠ¸ëœì­ì…˜ ê´€ë¦¬ | `backend/app/services/rag_service.py` |
| **Repository Layer** | ë°ì´í„° ì ‘ê·¼, ì¿¼ë¦¬ ìµœì í™” | `backend/app/repositories/rag_repository.py` |
| **Model Layer** | ë°ì´í„° ëª¨ë¸, ìŠ¤í‚¤ë§ˆ ì •ì˜ | `backend/app/models/rag_chunk.py` |

---

## 3. ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

### 3.1 Parser Service

#### ì±…ì„
- PDF/HWP ë¬¸ì„œë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ íŒŒì‹±
- ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ
- ì²­í¬ ìƒì„± ë° ê²€ì¦

#### ì¸í„°í˜ì´ìŠ¤

```python
# backend/app/services/parser_service.py

from typing import List, Dict, Any
from pathlib import Path
from pydantic import BaseModel

class ParsedChunk(BaseModel):
    content: str
    metadata: Dict[str, Any]
    page_number: Optional[int]
    
class ParserService:
    """ë¬¸ì„œ íŒŒì‹± ì„œë¹„ìŠ¤"""
    
    async def parse_document(
        self, 
        file_path: Path,
        document_type: str  # "curriculum" or "school_plan"
    ) -> List[ParsedChunk]:
        """
        ë¬¸ì„œë¥¼ íŒŒì‹±í•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
            document_type: ë¬¸ì„œ ìœ í˜•
            
        Returns:
            ParsedChunk ë¦¬ìŠ¤íŠ¸
            
        Raises:
            FileNotFoundError: íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ
            ParseError: íŒŒì‹± ì‹¤íŒ¨
            ValidationError: ë©”íƒ€ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨
        """
        pass
    
    async def extract_metadata(
        self,
        file_path: Path,
        content: str
    ) -> Dict[str, Any]:
        """
        ë¬¸ì„œì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        
        Returns:
            METADATA_SCHEMA.mdì— ì •ì˜ëœ í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„°
        """
        pass
    
    def validate_chunk(self, chunk: ParsedChunk) -> bool:
        """ì²­í¬ ìœ íš¨ì„± ê²€ì¦"""
        pass
```

#### êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

**PDF íŒŒì‹± (PyMuPDF)**
```python
import fitz  # PyMuPDF
import re

async def _parse_pdf(self, file_path: Path) -> List[ParsedChunk]:
    """PDF íŒŒì‹± êµ¬í˜„"""
    chunks = []
    doc = fitz.open(file_path)
    
    for page_num, page in enumerate(doc, start=1):
        text = page.get_text()
        
        # ì„±ì·¨ê¸°ì¤€ ì½”ë“œ íŒ¨í„´ ê°ì§€
        achievement_codes = re.findall(r'\[(\d+[ê°€-í£]+\d+-\d+)\]', text)
        
        if achievement_codes:
            # ì„±ì·¨ê¸°ì¤€ ë‹¨ìœ„ë¡œ ì²­í‚¹
            for code in achievement_codes:
                chunk_content = self._extract_achievement_section(text, code)
                metadata = {
                    "curriculum_code": f"[{code}]",
                    "page_number": page_num,
                    "document_type": "ì„±ì·¨ê¸°ì¤€"
                }
                chunks.append(ParsedChunk(
                    content=chunk_content,
                    metadata=metadata,
                    page_number=page_num
                ))
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹ (1000ì ë‹¨ìœ„)
            for i in range(0, len(text), 1000):
                chunk_content = text[i:i+1000]
                metadata = {
                    "page_number": page_num,
                    "document_type": "ì¼ë°˜"
                }
                chunks.append(ParsedChunk(
                    content=chunk_content,
                    metadata=metadata,
                    page_number=page_num
                ))
    
    return chunks
```

**HWP íŒŒì‹± (python-hwp)**
```python
from hwp5 import hwp5txt

async def _parse_hwp(self, file_path: Path) -> List[ParsedChunk]:
    """HWP íŒŒì‹± êµ¬í˜„"""
    # HWPë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    text = hwp5txt(str(file_path))
    
    # í•™êµ ê³„íš ë¬¸ì„œëŠ” ì„¹ì…˜ ë‹¨ìœ„ë¡œ ì²­í‚¹
    sections = self._split_by_sections(text)
    
    chunks = []
    for section_title, section_content in sections:
        metadata = {
            "document_type": self._classify_section(section_title),
            "section_title": section_title
        }
        chunks.append(ParsedChunk(
            content=section_content,
            metadata=metadata
        ))
    
    return chunks
```

### 3.2 RAG Service

#### ì±…ì„
- ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬
- ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
- LLM ë‹µë³€ ìƒì„±
- ê·¼ê±° ì¸ìš© ê´€ë¦¬

#### ì¸í„°í˜ì´ìŠ¤

```python
# backend/app/services/rag_service.py

from typing import List, Optional
from pydantic import BaseModel

class RAGQuery(BaseModel):
    """RAG ì§ˆì˜ ìš”ì²­"""
    query: str
    filters: Optional[Dict[str, Any]] = None
    top_k: int = 5
    include_sources: bool = True

class RAGSource(BaseModel):
    """ê²€ìƒ‰ëœ ì†ŒìŠ¤"""
    chunk_id: str
    content: str
    score: float
    metadata: Dict[str, Any]
    page_number: Optional[int]

class RAGResponse(BaseModel):
    """RAG ì‘ë‹µ"""
    answer: str
    sources: List[RAGSource]
    confidence: float
    processing_time_ms: int

class RAGService:
    """RAG ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        llm_client: LLMClient,
        db: Session
    ):
        self.vector_store = vector_store
        self.llm_client = llm_client
        self.db = db
    
    async def query(self, request: RAGQuery, user_id: str) -> RAGResponse:
        """
        RAG ì§ˆì˜ ì²˜ë¦¬
        
        1. ì§ˆì˜ ì„ë² ë”©
        2. ë²¡í„° ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í•„í„°ë§)
        3. ì¬ìˆœìœ„í™” (Re-ranking)
        4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        5. ë‹µë³€ ìƒì„±
        6. ì¸ìš© ì¶”ê°€
        7. ë¡œê·¸ ì €ì¥
        
        Args:
            request: RAG ì§ˆì˜ ìš”ì²­
            user_id: ì‚¬ìš©ì ID
            
        Returns:
            RAG ì‘ë‹µ
            
        Raises:
            VectorSearchError: ê²€ìƒ‰ ì‹¤íŒ¨
            LLMError: LLM í˜¸ì¶œ ì‹¤íŒ¨
            ValidationError: ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨
        """
        start_time = time.time()
        
        try:
            # 1. ì§ˆì˜ ì„ë² ë”©
            query_embedding = await self._embed_query(request.query)
            
            # 2. ë²¡í„° ê²€ìƒ‰
            search_results = await self._search_vectors(
                query_embedding,
                filters=request.filters,
                top_k=request.top_k
            )
            
            # 3. ì¬ìˆœìœ„í™”
            reranked_results = await self._rerank(request.query, search_results)
            
            # 4. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(request.query, reranked_results)
            
            # 5. ë‹µë³€ ìƒì„±
            answer = await self._generate_answer(prompt)
            
            # 6. ì¸ìš© ì¶”ê°€
            answer_with_citations = self._add_citations(answer, reranked_results)
            
            # 7. ë¡œê·¸ ì €ì¥
            await self._log_query(user_id, request, answer_with_citations)
            
            processing_time = int((time.time() - start_time) * 1000)
            
            return RAGResponse(
                answer=answer_with_citations,
                sources=[self._to_source(r) for r in reranked_results],
                confidence=self._calculate_confidence(reranked_results),
                processing_time_ms=processing_time
            )
            
        except Exception as e:
            await self._log_error(user_id, request, str(e))
            raise
    
    async def _embed_query(self, query: str) -> List[float]:
        """ì§ˆì˜ ì„ë² ë”©"""
        pass
    
    async def _search_vectors(
        self,
        embedding: List[float],
        filters: Optional[Dict],
        top_k: int
    ) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰"""
        pass
    
    async def _rerank(
        self,
        query: str,
        results: List[SearchResult]
    ) -> List[SearchResult]:
        """ì¬ìˆœìœ„í™”"""
        pass
    
    def _build_prompt(
        self,
        query: str,
        sources: List[SearchResult]
    ) -> str:
        """LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        return f"""ë‹¹ì‹ ì€ êµìœ¡ê³¼ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

**ì¤‘ìš” ê·œì¹™:**
1. ì œê³µëœ ê·¼ê±°ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
2. ëª¨ë“  ì‚¬ì‹¤ì— ëŒ€í•´ <ì¶œì²˜: [chunk_id]> í˜•ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”
3. ê·¼ê±°ê°€ ë¶ˆì¶©ë¶„í•˜ë©´ "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”

**ê·¼ê±°:**
{self._format_sources(sources)}

**ì§ˆë¬¸:** {query}

**ë‹µë³€:**"""
    
    async def _generate_answer(self, prompt: str) -> str:
        """LLM ë‹µë³€ ìƒì„±"""
        pass
    
    def _add_citations(self, answer: str, sources: List[SearchResult]) -> str:
        """ì¸ìš© ì¶”ê°€"""
        pass
```

### 3.3 Vector Store Wrapper

#### ì±…ì„
- Qdrantì™€ì˜ í†µì‹ 
- ë©”íƒ€ë°ì´í„° í•„í„°ë§
- ë²¡í„° CRUD ì‘ì—…

#### ì¸í„°í˜ì´ìŠ¤

```python
# backend/app/services/vector_store.py

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter

class VectorStore:
    """ë²¡í„° ì €ì¥ì†Œ ë˜í¼"""
    
    def __init__(self, url: str, api_key: Optional[str] = None):
        self.client = QdrantClient(url=url, api_key=api_key)
        self.collection_name = "rag_chunks"
    
    async def initialize(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        self.client.recreate_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=3072,  # text-embedding-3-large dimension
                distance=Distance.COSINE
            )
        )
    
    async def upsert(
        self,
        chunk_id: str,
        embedding: List[float],
        metadata: Dict[str, Any],
        content: str
    ):
        """ë²¡í„° ì‚½ì…/ì—…ë°ì´íŠ¸"""
        point = PointStruct(
            id=chunk_id,
            vector=embedding,
            payload={
                "content": content,
                **metadata
            }
        )
        self.client.upsert(
            collection_name=self.collection_name,
            points=[point]
        )
    
    async def search(
        self,
        query_vector: List[float],
        filters: Optional[Dict[str, Any]] = None,
        top_k: int = 5
    ) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰"""
        # ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±
        qdrant_filter = self._build_filter(filters) if filters else None
        
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            query_filter=qdrant_filter,
            limit=top_k
        )
        
        return [
            SearchResult(
                chunk_id=str(r.id),
                content=r.payload["content"],
                score=r.score,
                metadata=r.payload
            )
            for r in results
        ]
    
    def _build_filter(self, filters: Dict[str, Any]) -> Filter:
        """ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±"""
        from qdrant_client.models import FieldCondition, MatchValue
        
        conditions = []
        for key, value in filters.items():
            conditions.append(
                FieldCondition(
                    key=key,
                    match=MatchValue(value=value)
                )
            )
        
        return Filter(must=conditions)
```

---

## 4. ë°ì´í„° íë¦„

### 4.1 ë¬¸ì„œ ì¸ë±ì‹± í”Œë¡œìš°

```
[ì‚¬ìš©ì] â†’ [POST /api/v1/rag/index]
    â†“
[API Layer] íŒŒì¼ ì—…ë¡œë“œ ê²€ì¦
    â†“
[Parser Service] ë¬¸ì„œ íŒŒì‹±
    â”œâ”€ PDF/HWP ì½ê¸°
    â”œâ”€ êµ¬ì¡° ë¶„ì„
    â”œâ”€ ì²­í¬ ìƒì„±
    â””â”€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    â†“
[Embedding Service] ì„ë² ë”© ìƒì„±
    â””â”€ OpenAI API í˜¸ì¶œ
    â†“
[Vector Store] ë²¡í„° ì €ì¥
    â””â”€ Qdrantì— upsert
    â†“
[PostgreSQL] ë©”íƒ€ë°ì´í„° ì €ì¥
    â””â”€ rag_chunks í…Œì´ë¸”ì— INSERT
    â†“
[Response] {"status": "indexed", "chunks_count": 42}
```

### 4.2 ì§ˆì˜ ì‘ë‹µ í”Œë¡œìš°

```
[ì‚¬ìš©ì] â†’ [POST /api/v1/rag/query]
    â†“
[API Layer] ìš”ì²­ ê²€ì¦
    â†“
[RAG Service] ì§ˆì˜ ì²˜ë¦¬
    â”œâ”€ [1] ì§ˆì˜ ì„ë² ë”©
    â”‚   â””â”€ OpenAI Embedding API
    â”œâ”€ [2] ë²¡í„° ê²€ìƒ‰
    â”‚   â””â”€ Qdrant ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° í•„í„°ë§)
    â”œâ”€ [3] ì¬ìˆœìœ„í™” (ì„ íƒ)
    â”‚   â””â”€ Cross-Encoder ëª¨ë¸
    â”œâ”€ [4] í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    â”‚   â””â”€ ê·¼ê±° + ì§ˆë¬¸ ì¡°í•©
    â”œâ”€ [5] LLM í˜¸ì¶œ
    â”‚   â””â”€ OpenAI GPT-4 API
    â””â”€ [6] ì¸ìš© ì¶”ê°€
        â””â”€ <ì¶œì²˜: ...> íƒœê·¸ ì‚½ì…
    â†“
[PostgreSQL] ë¡œê·¸ ì €ì¥
    â””â”€ rag_query_logs í…Œì´ë¸”ì— INSERT
    â†“
[Response] {"answer": "...", "sources": [...]}
```

### 4.3 íŠ¸ëœì­ì…˜ ì²˜ë¦¬

```python
# ì¸ë±ì‹± íŠ¸ëœì­ì…˜
async def index_document(file_path: Path, db: Session):
    try:
        # 1. íŒŒì‹± (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)
        chunks = await parser_service.parse_document(file_path)
        
        # 2. DB íŠ¸ëœì­ì…˜ ì‹œì‘
        async with db.begin():
            # 2-1. ë©”íƒ€ë°ì´í„° ì €ì¥
            for chunk in chunks:
                db_chunk = RAGChunk(**chunk.dict())
                db.add(db_chunk)
            
            # 2-2. ì„ë² ë”© ìƒì„± (ì™¸ë¶€ API)
            embeddings = await embedding_service.embed_batch(
                [c.content for c in chunks]
            )
            
            # 2-3. ë²¡í„° ì €ì¥ (ì™¸ë¶€ DB)
            await vector_store.upsert_batch(chunks, embeddings)
            
            # ëª¨ë“  ì‘ì—… ì„±ê³µ ì‹œ ì»¤ë°‹
            await db.commit()
            
    except Exception as e:
        # ë¡¤ë°±
        await db.rollback()
        # ë²¡í„° DB ì •ë¦¬ (ë³´ìƒ íŠ¸ëœì­ì…˜)
        await vector_store.delete_batch([c.chunk_id for c in chunks])
        raise
```

---

## 5. ì˜ˆì™¸ ì²˜ë¦¬ ì „ëµ

### 5.1 ì˜ˆì™¸ ê³„ì¸µ êµ¬ì¡°

```python
# backend/app/exceptions/rag_exceptions.py

class RAGException(Exception):
    """RAG ì‹œìŠ¤í…œ ê¸°ë³¸ ì˜ˆì™¸"""
    pass

class ParseError(RAGException):
    """íŒŒì‹± ì‹¤íŒ¨"""
    pass

class ValidationError(RAGException):
    """ê²€ì¦ ì‹¤íŒ¨"""
    pass

class VectorSearchError(RAGException):
    """ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨"""
    pass

class LLMError(RAGException):
    """LLM í˜¸ì¶œ ì‹¤íŒ¨"""
    pass

class QuotaExceededError(LLMError):
    """API í• ë‹¹ëŸ‰ ì´ˆê³¼"""
    pass
```

### 5.2 ì¬ì‹œë„ ë¡œì§

```python
from tenacity import retry, stop_after_attempt, wait_exponential

class RAGService:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(VectorSearchError)
    )
    async def _search_vectors(self, ...):
        """ë²¡í„° ê²€ìƒ‰ (ì¬ì‹œë„ í¬í•¨)"""
        try:
            return await self.vector_store.search(...)
        except Exception as e:
            logger.error(f"Vector search failed: {e}")
            raise VectorSearchError(str(e))
```

### 5.3 íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬

```python
import asyncio

async def query_with_timeout(request: RAGQuery) -> RAGResponse:
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” ì§ˆì˜"""
    try:
        return await asyncio.wait_for(
            rag_service.query(request),
            timeout=30.0  # 30ì´ˆ
        )
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=504,
            detail="Query processing timeout"
        )
```

---

## 6. ë ˆê±°ì‹œ í†µí•©

### 6.1 ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ì˜ ì—°ë™ ì§€ì 

| ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ | ì—°ë™ ë°©ì‹ | ë°ì´í„° íë¦„ |
|-------------|----------|-----------|
| **Curriculum Model** | Foreign Key | `rag_chunks.curriculum_id â†’ curriculums.curriculum_id` |
| **Node Model** | Foreign Key | `rag_chunks.node_id â†’ nodes.node_id` (ì„ íƒ) |
| **NodeGraph.tsx** | REST API | `GET /api/v1/rag/related-nodes?node_id=xxx` |
| **AIAssistant.tsx** | REST API | `POST /api/v1/rag/query` |

### 6.2 ë°ì´í„° ëª¨ë¸ í™•ì¥

```python
# backend/app/models/rag_chunk.py

from sqlalchemy import Column, String, Text, JSON, Float, ForeignKey, DateTime
from sqlalchemy.orm import relationship
from backend.app.models.base import Base
import uuid

class RAGChunk(Base):
    __tablename__ = "rag_chunks"
    
    # ê¸°ë³¸ í•„ë“œ
    chunk_id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    content = Column(Text, nullable=False)
    metadata = Column(JSON, nullable=False)
    
    # ì„ë² ë”© ì •ë³´
    embedding_model = Column(String(100), default="text-embedding-3-large")
    embedding_dimension = Column(Integer, default=3072)
    
    # ë ˆê±°ì‹œ ì—°ë™
    curriculum_id = Column(String(36), ForeignKey("curriculums.curriculum_id"), nullable=True)
    node_id = Column(String(36), ForeignKey("nodes.node_id"), nullable=True)
    
    # ê´€ê³„
    curriculum = relationship("Curriculum", back_populates="rag_chunks")
    node = relationship("Node", back_populates="rag_chunks")
    
    # íƒ€ì„ìŠ¤íƒ¬í”„
    created_at = Column(DateTime, server_default=func.now())
    updated_at = Column(DateTime, server_default=func.now(), onupdate=func.now())
```

### 6.3 í”„ë¡ íŠ¸ì—”ë“œ í†µí•©

```typescript
// MATHESIS-LAB_FRONT/services/ragService.ts

export interface RAGQueryRequest {
  query: string;
  filters?: {
    curriculum_id?: string;
    node_id?: string;
    policy_version?: string;
    scope_type?: 'NATIONAL' | 'SCHOOL';
  };
  top_k?: number;
}

export interface RAGSource {
  chunk_id: string;
  content: string;
  score: number;
  metadata: Record<string, any>;
}

export interface RAGQueryResponse {
  answer: string;
  sources: RAGSource[];
  confidence: number;
  processing_time_ms: number;
}

export const queryRAG = async (request: RAGQueryRequest): Promise<RAGQueryResponse> => {
  const response = await fetch('/api/v1/rag/query', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    credentials: 'include',
    body: JSON.stringify(request)
  });
  
  if (!response.ok) {
    throw new Error('RAG query failed');
  }
  
  return response.json();
};
```

---

## 7. ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­

### 7.1 ì‘ë‹µ ì‹œê°„

| ì‘ì—… | ëª©í‘œ | ìµœëŒ€ í—ˆìš© |
|------|------|----------|
| ì§ˆì˜ ì‘ë‹µ (P95) | < 3ì´ˆ | < 5ì´ˆ |
| ë¬¸ì„œ ì¸ë±ì‹± (100í˜ì´ì§€) | < 2ë¶„ | < 5ë¶„ |
| ë²¡í„° ê²€ìƒ‰ | < 500ms | < 1ì´ˆ |

### 7.2 ì²˜ë¦¬ëŸ‰

- **ë™ì‹œ ì§ˆì˜**: 10 QPS (Queries Per Second)
- **ì¼ì¼ ì§ˆì˜**: ìµœëŒ€ 10,000ê±´
- **ì¸ë±ì‹±**: ì‹œê°„ë‹¹ 100ê°œ ë¬¸ì„œ

### 7.3 ìµœì í™” ì „ëµ

```python
# 1. ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬
async def embed_batch(texts: List[str], batch_size: int = 100):
    """ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ API í˜¸ì¶œ ìµœì†Œí™”"""
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_embeddings = await openai_client.embeddings.create(
            model="text-embedding-3-large",
            input=batch
        )
        embeddings.extend([e.embedding for e in batch_embeddings.data])
    return embeddings

# 2. ìºì‹±
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_embedding_cached(text: str) -> List[float]:
    """ìì£¼ ì‚¬ìš©ë˜ëŠ” ì§ˆì˜ ì„ë² ë”© ìºì‹±"""
    return openai_client.embeddings.create(
        model="text-embedding-3-large",
        input=text
    ).data[0].embedding
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ì‘ì„±ì¼**: 2025-11-20  
**ì‘ì„±ì**: MATHESIS LAB ê°œë°œíŒ€  
**ê²€í† ì**: [ê²€í†  í•„ìš”]
