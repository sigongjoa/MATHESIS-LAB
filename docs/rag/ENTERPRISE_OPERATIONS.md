# RAG ì‹œìŠ¤í…œ ì—”í„°í”„ë¼ì´ì¦ˆ ìš´ì˜ ì„¤ê³„ì„œ

## ğŸ“‹ ëª©ì°¨
1. [ë¹„ë™ê¸° ì²˜ë¦¬ ì•„í‚¤í…ì²˜](#1-ë¹„ë™ê¸°-ì²˜ë¦¬-ì•„í‚¤í…ì²˜)
2. [DB ìŠ¤í‚¤ë§ˆ ìµœì í™”](#2-db-ìŠ¤í‚¤ë§ˆ-ìµœì í™”)
3. [í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ](#3-í•˜ì´ë¸Œë¦¬ë“œ-ê²€ìƒ‰-ì „ëµ)
4. [ìŠ¤íŠ¸ë¦¬ë° API ì„¤ê³„](#4-ìŠ¤íŠ¸ë¦¬ë°-api-ì„¤ê³„)
5. [ë¹„ìš© ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§](#5-ë¹„ìš©-ê´€ë¦¬-ë°-ëª¨ë‹ˆí„°ë§)
6. [ë°ì´í„° ë™ê¸°í™” ì „ëµ](#6-ë°ì´í„°-ë™ê¸°í™”-ì „ëµ)
7. [ìë™í™”ëœ í’ˆì§ˆ í‰ê°€](#7-ìë™í™”ëœ-í’ˆì§ˆ-í‰ê°€)

---

## 1. ë¹„ë™ê¸° ì²˜ë¦¬ ì•„í‚¤í…ì²˜

### 1.1 Task Queue ì„¤ê³„ (Celery + Redis)

#### ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ POST /rag/index
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI    â”‚
â”‚  (API)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ 1. Create Job
       â”‚ 2. Enqueue Task
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis     â”‚â—„â”€â”€â”€â”€â”€â”¤   Celery    â”‚
â”‚  (Broker)   â”‚      â”‚   Worker    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ 3. Process
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Parser     â”‚
                     â”‚  Embedding  â”‚
                     â”‚  Vector DB  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### êµ¬í˜„

```python
# backend/app/celery_app.py

from celery import Celery
from backend.app.core.config import settings

celery_app = Celery(
    "rag_tasks",
    broker=settings.CELERY_BROKER_URL,  # redis://localhost:6379/0
    backend=settings.CELERY_RESULT_BACKEND  # redis://localhost:6379/1
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='Asia/Seoul',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=3600,  # 1ì‹œê°„
    task_soft_time_limit=3300,  # 55ë¶„
)
```

```python
# backend/app/tasks/indexing_tasks.py

from celery import Task
from backend.app.celery_app import celery_app
from backend.app.services.parser_service import ParserService
from backend.app.services.vector_store import VectorStore
from backend.app.models.rag_chunk import RAGDocument, RAGIndexingJob
from sqlalchemy.orm import Session

class CallbackTask(Task):
    """ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ë² ì´ìŠ¤ íƒœìŠ¤í¬"""
    
    def on_success(self, retval, task_id, args, kwargs):
        """ì„±ê³µ ì‹œ ì½œë°±"""
        job_id = kwargs.get('job_id')
        # DB ì—…ë°ì´íŠ¸
        
    def on_failure(self, exc, task_id, args, kwargs, einfo):
        """ì‹¤íŒ¨ ì‹œ ì½œë°±"""
        job_id = kwargs.get('job_id')
        # ì—ëŸ¬ ë¡œê¹…

@celery_app.task(
    bind=True,
    base=CallbackTask,
    max_retries=3,
    default_retry_delay=60
)
def index_document_task(
    self,
    document_id: str,
    file_path: str,
    job_id: str
):
    """
    ë¬¸ì„œ ì¸ë±ì‹± ë¹„ë™ê¸° íƒœìŠ¤í¬
    
    Args:
        document_id: ë¬¸ì„œ ID
        file_path: íŒŒì¼ ê²½ë¡œ
        job_id: ì‘ì—… ID
    """
    from backend.app.db.session import SessionLocal
    
    db = SessionLocal()
    
    try:
        # 1. Job ìƒíƒœ ì—…ë°ì´íŠ¸: PROCESSING
        job = db.query(RAGIndexingJob).filter_by(job_id=job_id).first()
        job.status = "processing"
        job.current_step = "parsing"
        db.commit()
        
        # 2. ë¬¸ì„œ íŒŒì‹±
        parser = ParserService()
        chunks = await parser.parse_document(Path(file_path), "curriculum")
        
        job.chunks_total = len(chunks)
        job.current_step = "embedding"
        db.commit()
        
        # 3. ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬)
        embeddings = []
        batch_size = 100
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i+batch_size]
            batch_embeddings = await embedding_service.embed_batch(
                [c.content for c in batch]
            )
            embeddings.extend(batch_embeddings)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            job.chunks_processed = i + len(batch)
            job.progress = int((job.chunks_processed / job.chunks_total) * 100)
            db.commit()
            
            # Celery ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            self.update_state(
                state='PROGRESS',
                meta={
                    'current': job.chunks_processed,
                    'total': job.chunks_total,
                    'step': 'embedding'
                }
            )
        
        job.current_step = "indexing"
        db.commit()
        
        # 4. ë²¡í„° DB ì €ì¥
        vector_store = VectorStore()
        for chunk, embedding in zip(chunks, embeddings):
            await vector_store.upsert(
                chunk.chunk_id,
                embedding,
                chunk.metadata,
                chunk.content
            )
            
            # DBì— ì²­í¬ ë©”íƒ€ë°ì´í„° ì €ì¥
            db_chunk = RAGChunk(
                chunk_id=chunk.chunk_id,
                document_id=document_id,
                content=chunk.content,
                metadata=chunk.metadata,
                # ìµœì í™”: ìì£¼ ì¿¼ë¦¬ë˜ëŠ” í•„ë“œ ìŠ¹ê²©
                policy_version=chunk.metadata.get("policy_version"),
                scope_type=chunk.metadata.get("scope_type"),
                institution_id=chunk.metadata.get("institution_id"),
                grade_level=chunk.metadata.get("grade_level"),
                domain=chunk.metadata.get("domain")
            )
            db.add(db_chunk)
        
        db.commit()
        
        # 5. Job ì™„ë£Œ
        job.status = "completed"
        job.progress = 100
        job.completed_at = datetime.now()
        db.commit()
        
        return {
            "status": "completed",
            "chunks_created": len(chunks)
        }
        
    except Exception as e:
        # ì—ëŸ¬ ì²˜ë¦¬
        job.status = "failed"
        job.error_message = str(e)
        job.error_stack = traceback.format_exc()
        db.commit()
        
        # ì¬ì‹œë„
        raise self.retry(exc=e)
        
    finally:
        db.close()
```

#### API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •

```python
# backend/app/api/v1/endpoints/rag.py

from backend.app.tasks.indexing_tasks import index_document_task

@router.post("/index", status_code=202)
async def index_document(
    file: UploadFile = File(...),
    document_type: str = Form(...),
    metadata: str = Form(...),
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ë¬¸ì„œ ì¸ë±ì‹± (ë¹„ë™ê¸°)
    
    Returns:
        202 Accepted - Job ID ë°˜í™˜
    """
    # 1. íŒŒì¼ ì €ì¥
    file_path = await save_uploaded_file(file)
    
    # 2. Document ë ˆì½”ë“œ ìƒì„±
    document = RAGDocument(
        file_path=str(file_path),
        file_name=file.filename,
        document_type=document_type,
        status="pending",
        **json.loads(metadata)
    )
    db.add(document)
    db.commit()
    
    # 3. Job ìƒì„±
    job = RAGIndexingJob(
        document_id=document.document_id,
        status="pending"
    )
    db.add(job)
    db.commit()
    
    # 4. Celery íƒœìŠ¤í¬ ì‹¤í–‰
    task = index_document_task.delay(
        document_id=document.document_id,
        file_path=str(file_path),
        job_id=job.job_id
    )
    
    return {
        "status": "accepted",
        "job_id": job.job_id,
        "task_id": task.id,
        "estimated_time_seconds": 120
    }
```

### 1.2 WebSocket ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ì•Œë¦¼

```python
# backend/app/api/v1/endpoints/websocket.py

from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict

class ConnectionManager:
    """WebSocket ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, job_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active_connections[job_id] = websocket
    
    def disconnect(self, job_id: str):
        if job_id in self.active_connections:
            del self.active_connections[job_id]
    
    async def send_progress(self, job_id: str, data: dict):
        if job_id in self.active_connections:
            await self.active_connections[job_id].send_json(data)

manager = ConnectionManager()

@router.websocket("/ws/indexing/{job_id}")
async def websocket_indexing_progress(
    websocket: WebSocket,
    job_id: str,
    db: Session = Depends(get_db)
):
    """ì¸ë±ì‹± ì§„í–‰ ìƒí™© WebSocket"""
    await manager.connect(job_id, websocket)
    
    try:
        while True:
            # DBì—ì„œ ìµœì‹  ìƒíƒœ ì¡°íšŒ
            job = db.query(RAGIndexingJob).filter_by(job_id=job_id).first()
            
            await manager.send_progress(job_id, {
                "status": job.status,
                "progress": job.progress,
                "current_step": job.current_step,
                "chunks_processed": job.chunks_processed,
                "chunks_total": job.chunks_total
            })
            
            if job.status in ["completed", "failed"]:
                break
            
            await asyncio.sleep(1)
            
    except WebSocketDisconnect:
        manager.disconnect(job_id)
```

---

## 2. DB ìŠ¤í‚¤ë§ˆ ìµœì í™”

### 2.1 ê°œì„ ëœ rag_chunks í…Œì´ë¸”

```sql
CREATE TABLE rag_chunks (
    chunk_id VARCHAR(36) PRIMARY KEY DEFAULT (UUID()),
    document_id VARCHAR(36) NOT NULL,
    
    -- ì²­í¬ ë‚´ìš©
    content TEXT NOT NULL,
    content_hash VARCHAR(64) NOT NULL,
    chunk_index INTEGER NOT NULL,
    
    -- â­ ë©”íƒ€ë°ì´í„° (JSON) - ëœ ì¤‘ìš”í•œ í•„ë“œë§Œ
    metadata JSON NOT NULL,
    
    -- â­ ìì£¼ ì¿¼ë¦¬ë˜ëŠ” í•„ë“œë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ìŠ¹ê²© (JSONì—ì„œ êº¼ëƒ„)
    policy_version VARCHAR(20) NOT NULL,
    scope_type VARCHAR(20) NOT NULL,
    institution_id VARCHAR(100),
    grade_level VARCHAR(20),
    domain VARCHAR(50),
    subject VARCHAR(50),
    curriculum_code VARCHAR(50),
    
    -- ì„ë² ë”© ì •ë³´
    embedding_model VARCHAR(100) NOT NULL DEFAULT 'text-embedding-3-large',
    embedding_dimension INTEGER NOT NULL DEFAULT 3072,
    vector_id VARCHAR(100),
    
    -- ë ˆê±°ì‹œ ì‹œìŠ¤í…œ ì—°ë™
    curriculum_id VARCHAR(36),
    node_id VARCHAR(36),
    
    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- ì™¸ë˜ í‚¤
    FOREIGN KEY (document_id) REFERENCES rag_documents(document_id) ON DELETE CASCADE,
    FOREIGN KEY (curriculum_id) REFERENCES curriculums(curriculum_id) ON DELETE SET NULL,
    FOREIGN KEY (node_id) REFERENCES nodes(node_id) ON DELETE SET NULL,
    
    -- â­ ìµœì í™”ëœ ì¸ë±ìŠ¤
    INDEX idx_document_id (document_id),
    INDEX idx_curriculum_id (curriculum_id),
    INDEX idx_node_id (node_id),
    INDEX idx_content_hash (content_hash),
    
    -- â­ ë³µí•© ì¸ë±ìŠ¤ (ìì£¼ ì‚¬ìš©ë˜ëŠ” í•„í„° ì¡°í•©)
    INDEX idx_filter_national (policy_version, scope_type, grade_level, domain),
    INDEX idx_filter_school (policy_version, scope_type, institution_id),
    INDEX idx_curriculum_code (curriculum_code),
    
    -- ë³µí•© ì¸ë±ìŠ¤
    UNIQUE INDEX idx_document_chunk (document_id, chunk_index)
);
```

### 2.2 Full-Text Search ì¸ë±ìŠ¤

```sql
-- MySQL Full-Text Search
ALTER TABLE rag_chunks ADD FULLTEXT INDEX idx_content_fulltext (content);

-- ì‚¬ìš© ì˜ˆì‹œ
SELECT * FROM rag_chunks
WHERE MATCH(content) AGAINST('ìµœëŒ€ê³µì•½ìˆ˜ ì†Œì¸ìˆ˜ë¶„í•´' IN NATURAL LANGUAGE MODE)
AND policy_version = '2022ê°œì •'
LIMIT 10;
```

---

## 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ

### 3.1 ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸

```python
# backend/app/services/hybrid_search_service.py

from typing import List
from dataclasses import dataclass

@dataclass
class SearchResult:
    chunk_id: str
    content: str
    score: float
    metadata: dict
    search_type: str  # 'vector', 'keyword', 'hybrid'

class HybridSearchService:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„œë¹„ìŠ¤"""
    
    def __init__(
        self,
        vector_store: VectorStore,
        db: Session
    ):
        self.vector_store = vector_store
        self.db = db
    
    async def search(
        self,
        query: str,
        filters: dict,
        top_k: int = 5,
        search_mode: str = "hybrid"  # 'vector', 'keyword', 'hybrid'
    ) -> List[SearchResult]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì§ˆì˜
            filters: ë©”íƒ€ë°ì´í„° í•„í„°
            top_k: ê²°ê³¼ ìˆ˜
            search_mode: ê²€ìƒ‰ ëª¨ë“œ
        """
        if search_mode == "vector":
            return await self._vector_search(query, filters, top_k)
        elif search_mode == "keyword":
            return await self._keyword_search(query, filters, top_k)
        else:  # hybrid
            return await self._hybrid_search(query, filters, top_k)
    
    async def _vector_search(
        self,
        query: str,
        filters: dict,
        top_k: int
    ) -> List[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ (ì˜ë¯¸ì  ìœ ì‚¬ë„)"""
        # 1. ì§ˆì˜ ì„ë² ë”©
        embedding = await embedding_service.embed(query)
        
        # 2. Qdrant ê²€ìƒ‰
        results = await self.vector_store.search(
            query_vector=embedding,
            filters=filters,
            top_k=top_k
        )
        
        return [
            SearchResult(
                chunk_id=r.chunk_id,
                content=r.content,
                score=r.score,
                metadata=r.metadata,
                search_type="vector"
            )
            for r in results
        ]
    
    async def _keyword_search(
        self,
        query: str,
        filters: dict,
        top_k: int
    ) -> List[SearchResult]:
        """í‚¤ì›Œë“œ ê²€ìƒ‰ (Full-Text Search)"""
        from sqlalchemy import text, and_
        
        # í•„í„° ì¡°ê±´ êµ¬ì„±
        filter_conditions = []
        if filters.get("policy_version"):
            filter_conditions.append(
                RAGChunk.policy_version == filters["policy_version"]
            )
        if filters.get("scope_type"):
            filter_conditions.append(
                RAGChunk.scope_type == filters["scope_type"]
            )
        if filters.get("institution_id"):
            filter_conditions.append(
                RAGChunk.institution_id == filters["institution_id"]
            )
        
        # Full-Text Search
        results = self.db.query(RAGChunk).filter(
            and_(
                text(f"MATCH(content) AGAINST(:query IN NATURAL LANGUAGE MODE)"),
                *filter_conditions
            )
        ).params(query=query).limit(top_k).all()
        
        return [
            SearchResult(
                chunk_id=r.chunk_id,
                content=r.content,
                score=0.5,  # Full-textëŠ” ì ìˆ˜ ê³„ì‚° ë‹¤ë¦„
                metadata=r.metadata,
                search_type="keyword"
            )
            for r in results
        ]
    
    async def _hybrid_search(
        self,
        query: str,
        filters: dict,
        top_k: int
    ) -> List[SearchResult]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Reciprocal Rank Fusion)
        
        1. ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        2. í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼
        3. RRFë¡œ ì¬ìˆœìœ„í™”
        """
        # 1. ë‘ ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰
        vector_results, keyword_results = await asyncio.gather(
            self._vector_search(query, filters, top_k * 2),
            self._keyword_search(query, filters, top_k * 2)
        )
        
        # 2. Reciprocal Rank Fusion
        rrf_scores = {}
        k = 60  # RRF ìƒìˆ˜
        
        for rank, result in enumerate(vector_results, start=1):
            rrf_scores[result.chunk_id] = rrf_scores.get(result.chunk_id, 0) + 1 / (k + rank)
        
        for rank, result in enumerate(keyword_results, start=1):
            rrf_scores[result.chunk_id] = rrf_scores.get(result.chunk_id, 0) + 1 / (k + rank)
        
        # 3. ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        all_results = {r.chunk_id: r for r in vector_results + keyword_results}
        sorted_ids = sorted(rrf_scores.keys(), key=lambda x: rrf_scores[x], reverse=True)
        
        # 4. Top-K ë°˜í™˜
        final_results = []
        for chunk_id in sorted_ids[:top_k]:
            result = all_results[chunk_id]
            result.score = rrf_scores[chunk_id]
            result.search_type = "hybrid"
            final_results.append(result)
        
        return final_results
```

---

## 4. ìŠ¤íŠ¸ë¦¬ë° API ì„¤ê³„

### 4.1 Server-Sent Events (SSE) ì—”ë“œí¬ì¸íŠ¸

```python
# backend/app/api/v1/endpoints/rag.py

from fastapi.responses import StreamingResponse
import json

@router.post("/query/stream")
async def query_rag_stream(
    request: RAGQuery,
    current_user: User = Depends(get_current_user)
):
    """
    RAG ì§ˆì˜ (ìŠ¤íŠ¸ë¦¬ë°)
    
    Returns:
        text/event-stream
    """
    async def event_generator():
        """SSE ì´ë²¤íŠ¸ ìƒì„±ê¸°"""
        query_id = str(uuid.uuid4())
        
        try:
            # 1. ì‹œì‘ ì´ë²¤íŠ¸
            yield f"data: {json.dumps({'type': 'start', 'query_id': query_id})}\n\n"
            
            # 2. ê²€ìƒ‰ ë‹¨ê³„
            yield f"data: {json.dumps({'type': 'status', 'step': 'searching'})}\n\n"
            
            embedding = await embedding_service.embed(request.query)
            results = await vector_store.search(embedding, request.filters, request.top_k)
            
            # 3. ì¶œì²˜ ì „ì†¡
            for result in results:
                yield f"data: {json.dumps({
                    'type': 'source',
                    'chunk_id': result.chunk_id,
                    'score': result.score,
                    'metadata': result.metadata
                })}\n\n"
            
            # 4. ìƒì„± ë‹¨ê³„
            yield f"data: {json.dumps({'type': 'status', 'step': 'generating'})}\n\n"
            
            # 5. LLM ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
            prompt = rag_service._build_prompt(request.query, results)
            
            async for chunk in llm_client.stream_generate(prompt):
                yield f"data: {json.dumps({
                    'type': 'token',
                    'content': chunk
                })}\n\n"
            
            # 6. ì™„ë£Œ ì´ë²¤íŠ¸
            yield f"data: {json.dumps({
                'type': 'done',
                'query_id': query_id,
                'confidence': 0.89
            })}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({
                'type': 'error',
                'message': str(e)
            })}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no"
        }
    )
```

### 4.2 í”„ë¡ íŠ¸ì—”ë“œ SSE í´ë¼ì´ì–¸íŠ¸

```typescript
// MATHESIS-LAB_FRONT/services/ragService.ts

export const queryRAGStream = async (
  request: RAGQueryRequest,
  onToken: (token: string) => void,
  onSource: (source: RAGSource) => void,
  onComplete: (queryId: string) => void,
  onError: (error: string) => void
) => {
  const response = await fetch('/api/v1/rag/query/stream', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    credentials: 'include',
    body: JSON.stringify(request)
  });

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n');

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = JSON.parse(line.slice(6));

        switch (data.type) {
          case 'token':
            onToken(data.content);
            break;
          case 'source':
            onSource(data);
            break;
          case 'done':
            onComplete(data.query_id);
            break;
          case 'error':
            onError(data.message);
            break;
        }
      }
    }
  }
};
```

---

## 5. ë¹„ìš© ê´€ë¦¬ ë° ëª¨ë‹ˆí„°ë§

### 5.1 ê°œì„ ëœ rag_query_logs í…Œì´ë¸”

```sql
CREATE TABLE rag_query_logs (
    log_id BIGINT PRIMARY KEY AUTO_INCREMENT,
    query_id VARCHAR(36) NOT NULL UNIQUE,
    
    -- ì‚¬ìš©ì ì •ë³´
    user_id VARCHAR(36) NOT NULL,
    
    -- ì§ˆì˜ ì •ë³´
    query_text TEXT NOT NULL,
    filters JSON,
    top_k INTEGER NOT NULL DEFAULT 5,
    
    -- ì‘ë‹µ ì •ë³´
    answer TEXT NOT NULL,
    sources JSON NOT NULL,
    confidence FLOAT,
    
    -- â­ ë¹„ìš© ì¶”ì 
    prompt_tokens INTEGER NOT NULL DEFAULT 0,
    completion_tokens INTEGER NOT NULL DEFAULT 0,
    total_tokens INTEGER NOT NULL DEFAULT 0,
    estimated_cost_usd DECIMAL(10, 6) NOT NULL DEFAULT 0.000000,
    
    -- â­ ì„±ëŠ¥ ë©”íŠ¸ë¦­
    processing_time_ms INTEGER NOT NULL,
    embedding_time_ms INTEGER,
    search_time_ms INTEGER,
    llm_time_ms INTEGER,
    
    -- â­ í’ˆì§ˆ ë©”íŠ¸ë¦­
    user_rating INTEGER CHECK (user_rating BETWEEN 1 AND 5),
    feedback_type VARCHAR(20),
    
    -- íƒ€ì„ìŠ¤íƒ¬í”„
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,
    
    -- ì™¸ë˜ í‚¤
    FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE,
    
    -- ì¸ë±ìŠ¤
    INDEX idx_user_id (user_id),
    INDEX idx_query_id (query_id),
    INDEX idx_created_at (created_at),
    INDEX idx_cost (estimated_cost_usd),
    
    -- ì „ë¬¸ ê²€ìƒ‰ ì¸ë±ìŠ¤
    FULLTEXT INDEX idx_query_text (query_text)
);
```

### 5.2 ë¹„ìš© ì¶”ì  ë¡œì§

```python
# backend/app/services/cost_tracker.py

class CostTracker:
    """LLM ë¹„ìš© ì¶”ì """
    
    # OpenAI ê°€ê²© (2025ë…„ ê¸°ì¤€)
    PRICING = {
        "text-embedding-3-large": {
            "input": 0.00013 / 1000  # per token
        },
        "gpt-4-turbo": {
            "input": 0.01 / 1000,
            "output": 0.03 / 1000
        }
    }
    
    @staticmethod
    def calculate_cost(
        model: str,
        prompt_tokens: int,
        completion_tokens: int = 0
    ) -> float:
        """ë¹„ìš© ê³„ì‚°"""
        pricing = CostTracker.PRICING.get(model, {})
        
        input_cost = prompt_tokens * pricing.get("input", 0)
        output_cost = completion_tokens * pricing.get("output", 0)
        
        return input_cost + output_cost
    
    @staticmethod
    async def log_usage(
        db: Session,
        query_id: str,
        model: str,
        prompt_tokens: int,
        completion_tokens: int
    ):
        """ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
        cost = CostTracker.calculate_cost(model, prompt_tokens, completion_tokens)
        
        log = db.query(RAGQueryLog).filter_by(query_id=query_id).first()
        log.prompt_tokens = prompt_tokens
        log.completion_tokens = completion_tokens
        log.total_tokens = prompt_tokens + completion_tokens
        log.estimated_cost_usd = cost
        db.commit()
```

### 5.3 ë¹„ìš© ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
# backend/app/services/cost_alert_service.py

class CostAlertService:
    """ë¹„ìš© ì•Œë¦¼ ì„œë¹„ìŠ¤"""
    
    DAILY_BUDGET_USD = 100.0
    HOURLY_BUDGET_USD = 10.0
    
    @staticmethod
    async def check_budget(db: Session):
        """ì˜ˆì‚° ì´ˆê³¼ í™•ì¸"""
        from datetime import datetime, timedelta
        
        # ì˜¤ëŠ˜ ë¹„ìš©
        today = datetime.now().date()
        daily_cost = db.query(
            func.sum(RAGQueryLog.estimated_cost_usd)
        ).filter(
            func.date(RAGQueryLog.created_at) == today
        ).scalar() or 0.0
        
        if daily_cost >= CostAlertService.DAILY_BUDGET_USD:
            await send_alert(
                f"âš ï¸ Daily budget exceeded: ${daily_cost:.2f}"
            )
            return False
        
        # ìµœê·¼ 1ì‹œê°„ ë¹„ìš©
        one_hour_ago = datetime.now() - timedelta(hours=1)
        hourly_cost = db.query(
            func.sum(RAGQueryLog.estimated_cost_usd)
        ).filter(
            RAGQueryLog.created_at >= one_hour_ago
        ).scalar() or 0.0
        
        if hourly_cost >= CostAlertService.HOURLY_BUDGET_USD:
            await send_alert(
                f"âš ï¸ Hourly budget exceeded: ${hourly_cost:.2f}"
            )
            return False
        
        return True
```

---

## 6. ë°ì´í„° ë™ê¸°í™” ì „ëµ

### 6.1 Change Data Capture (CDC) íŠ¸ë¦¬ê±°

```sql
-- ì›ë³¸ curriculum ì‚­ì œ ì‹œ RAG ì²­í¬ë„ ë¬´íš¨í™”
DELIMITER $$

CREATE TRIGGER sync_curriculum_delete
AFTER DELETE ON curriculums
FOR EACH ROW
BEGIN
    -- 1. ê´€ë ¨ ì²­í¬ë¥¼ soft delete
    UPDATE rag_chunks
    SET metadata = JSON_SET(metadata, '$.deleted', TRUE),
        updated_at = NOW()
    WHERE curriculum_id = OLD.curriculum_id;
    
    -- 2. ë²¡í„° DB ì‚­ì œ ì‘ì—… íì— ì¶”ê°€
    INSERT INTO rag_sync_queue (
        action,
        entity_type,
        entity_id,
        created_at
    ) VALUES (
        'delete_vectors',
        'curriculum',
        OLD.curriculum_id,
        NOW()
    );
END$$

-- ì›ë³¸ curriculum ìˆ˜ì • ì‹œ ì¬ì¸ë±ì‹± í•„ìš” í‘œì‹œ
CREATE TRIGGER sync_curriculum_update
AFTER UPDATE ON curriculums
FOR EACH ROW
BEGIN
    IF OLD.title != NEW.title OR OLD.description != NEW.description THEN
        INSERT INTO rag_sync_queue (
            action,
            entity_type,
            entity_id,
            created_at
        ) VALUES (
            'reindex',
            'curriculum',
            NEW.curriculum_id,
            NOW()
        );
    END IF;
END$$

DELIMITER ;
```

### 6.2 ë™ê¸°í™” í ì²˜ë¦¬ ì›Œì»¤

```python
# backend/app/tasks/sync_tasks.py

@celery_app.task
def process_sync_queue():
    """ë™ê¸°í™” í ì²˜ë¦¬"""
    from backend.app.db.session import SessionLocal
    
    db = SessionLocal()
    
    try:
        # ëŒ€ê¸° ì¤‘ì¸ ë™ê¸°í™” ì‘ì—… ì¡°íšŒ
        pending_syncs = db.query(RAGSyncQueue).filter_by(
            status="pending"
        ).limit(100).all()
        
        for sync in pending_syncs:
            try:
                if sync.action == "delete_vectors":
                    # ë²¡í„° DBì—ì„œ ì‚­ì œ
                    await vector_store.delete_by_metadata({
                        "curriculum_id": sync.entity_id
                    })
                
                elif sync.action == "reindex":
                    # ì¬ì¸ë±ì‹± ì‘ì—… ìƒì„±
                    # ...
                
                sync.status = "completed"
                sync.completed_at = datetime.now()
                
            except Exception as e:
                sync.status = "failed"
                sync.error_message = str(e)
            
            db.commit()
            
    finally:
        db.close()
```

---

## 7. ìë™í™”ëœ í’ˆì§ˆ í‰ê°€

### 7.1 CI/CD í†µí•© í’ˆì§ˆ í…ŒìŠ¤íŠ¸

```yaml
# .github/workflows/rag-quality-check.yml

name: RAG Quality Check

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  quality-test:
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install ragas arize-phoenix
      
      - name: Run Golden Set Tests
        run: |
          pytest backend/tests/quality/test_golden_set.py \
            --junitxml=reports/golden-set-results.xml \
            --html=reports/golden-set-report.html
      
      - name: Run Ragas Evaluation
        run: |
          python backend/tests/quality/run_ragas_eval.py \
            --output reports/ragas-scores.json
      
      - name: Check Quality Thresholds
        run: |
          python backend/tests/quality/check_thresholds.py \
            --ragas-file reports/ragas-scores.json \
            --min-faithfulness 0.8 \
            --min-answer-relevancy 0.7 \
            --min-context-recall 0.7
      
      - name: Upload Quality Report
        uses: actions/upload-artifact@v2
        with:
          name: quality-reports
          path: reports/
      
      - name: Comment PR with Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const scores = JSON.parse(fs.readFileSync('reports/ragas-scores.json'));
            
            const comment = `
            ## ğŸ“Š RAG Quality Check Results
            
            | Metric | Score | Threshold | Status |
            |--------|-------|-----------|--------|
            | Faithfulness | ${scores.faithfulness.toFixed(2)} | 0.80 | ${scores.faithfulness >= 0.8 ? 'âœ…' : 'âŒ'} |
            | Answer Relevancy | ${scores.answer_relevancy.toFixed(2)} | 0.70 | ${scores.answer_relevancy >= 0.7 ? 'âœ…' : 'âŒ'} |
            | Context Recall | ${scores.context_recall.toFixed(2)} | 0.70 | ${scores.context_recall >= 0.7 ? 'âœ…' : 'âŒ'} |
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
```

### 7.2 ìë™í™”ëœ í’ˆì§ˆ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

```python
# backend/tests/quality/run_ragas_eval.py

import argparse
import json
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from backend.app.services.rag_service import RAGService

async def run_evaluation(golden_set_path: str, output_path: str):
    """Ragas í‰ê°€ ì‹¤í–‰"""
    
    # Golden Set ë¡œë“œ
    with open(golden_set_path) as f:
        golden_set = json.load(f)
    
    # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„±
    rag_service = RAGService(...)
    
    test_dataset = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }
    
    for test_case in golden_set:
        # ì§ˆì˜ ì‹¤í–‰
        response = await rag_service.query(
            RAGQuery(query=test_case["query"]),
            user_id="test_user"
        )
        
        test_dataset["question"].append(test_case["query"])
        test_dataset["answer"].append(response.answer)
        test_dataset["contexts"].append([s.content for s in response.sources])
        test_dataset["ground_truth"].append(test_case["expected_answer"])
    
    # Ragas í‰ê°€
    result = evaluate(
        test_dataset,
        metrics=[faithfulness, answer_relevancy, context_recall, context_precision]
    )
    
    # ê²°ê³¼ ì €ì¥
    with open(output_path, 'w') as f:
        json.dump({
            "faithfulness": float(result["faithfulness"]),
            "answer_relevancy": float(result["answer_relevancy"]),
            "context_recall": float(result["context_recall"]),
            "context_precision": float(result["context_precision"]),
            "timestamp": datetime.now().isoformat()
        }, f, indent=2)
    
    print(f"âœ… Evaluation completed. Results saved to {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--golden-set", default="tests/fixtures/golden_set.json")
    parser.add_argument("--output", default="reports/ragas-scores.json")
    args = parser.parse_args()
    
    asyncio.run(run_evaluation(args.golden_set, args.output))
```

---

**ë¬¸ì„œ ë²„ì „**: 1.0  
**ì‘ì„±ì¼**: 2025-11-20  
**ì‘ì„±ì**: MATHESIS LAB ê°œë°œíŒ€  
**ê²€í† ì**: Enterprise Architecture Team
